{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2v3p21WyywB",
    "outputId": "91416a1c-c832-4b31-ad4d-8cd7359fd6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.41.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mlost+found\u001b[0m/  messages_20240520.xlsx  practicum_cb.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('messages_20240520.xlsx')\n",
    "\n",
    "# Preprocess data\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.dropna(subset=['Text'])  # Drop rows with missing text\n",
    "data['Text'] = data['Text'].astype(str)  # Ensure text column is string type\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Contact UUID</th>\n",
       "      <th>Contact Name</th>\n",
       "      <th>URN Scheme</th>\n",
       "      <th>URN Value</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Text</th>\n",
       "      <th>Attachments</th>\n",
       "      <th>Status</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>756111</th>\n",
       "      <td>2024-01-17 10:31:51</td>\n",
       "      <td>35ca175a-e695-43ab-b09a-4a7142b9c7e4</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27632508008</td>\n",
       "      <td>AFFIRM-2-part1</td>\n",
       "      <td>IN</td>\n",
       "      <td>👍🏽Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349469</th>\n",
       "      <td>2023-12-31 15:07:37</td>\n",
       "      <td>124f3e91-1ec8-4a15-8e31-73a312fa89b8</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27810071650</td>\n",
       "      <td>SCREENER-normal</td>\n",
       "      <td>IN</td>\n",
       "      <td>Tell me</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816143</th>\n",
       "      <td>2024-01-18 20:59:44</td>\n",
       "      <td>de4271c8-91ad-437d-ad5f-71bc6c747815</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27612420701</td>\n",
       "      <td>Stories-surprise</td>\n",
       "      <td>IN</td>\n",
       "      <td>Next 👉🏽</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269993</th>\n",
       "      <td>2023-12-21 20:10:56</td>\n",
       "      <td>5b2d2fcf-5d4e-4fed-9883-9533a54a094f</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27640365420</td>\n",
       "      <td>BETTER-menu</td>\n",
       "      <td>IN</td>\n",
       "      <td>👍🏽Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53587</th>\n",
       "      <td>2023-12-04 05:29:22</td>\n",
       "      <td>71499b9d-e50d-459b-afbe-5066424b0284</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27604023555</td>\n",
       "      <td>BETTER-breathing-menu</td>\n",
       "      <td>IN</td>\n",
       "      <td>🧠 Box</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259444</th>\n",
       "      <td>2023-12-20 17:32:16</td>\n",
       "      <td>61452fd2-ada6-4e24-9dcb-540c4218cb8f</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27641791761</td>\n",
       "      <td>BETTER-menu</td>\n",
       "      <td>IN</td>\n",
       "      <td>🌈 FanaFana Fundi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366139</th>\n",
       "      <td>2024-01-02 18:11:30</td>\n",
       "      <td>6ea2207e-ca08-4833-9442-f91d3509ab24</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27648192647</td>\n",
       "      <td>FORGET</td>\n",
       "      <td>IN</td>\n",
       "      <td>Forget</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132179</th>\n",
       "      <td>2023-12-09 18:50:33</td>\n",
       "      <td>074b79ef-fc5c-40eb-87f8-3180f00770f0</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27656610248</td>\n",
       "      <td>Main-Menu-04-ReducedMenu4Options</td>\n",
       "      <td>IN</td>\n",
       "      <td>Read stories 📘</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671513</th>\n",
       "      <td>2024-01-15 11:24:32</td>\n",
       "      <td>48bde2c6-d9a0-43ae-883c-ad1b0c592644</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27676679745</td>\n",
       "      <td>Main-Menu-04-ReducedMenu4Options</td>\n",
       "      <td>IN</td>\n",
       "      <td>Health quizzes 🧠</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122202</th>\n",
       "      <td>2023-12-08 20:31:25</td>\n",
       "      <td>e836a6f2-7b12-4eb9-bf81-c7372a517c3f</td>\n",
       "      <td>friend</td>\n",
       "      <td>tel</td>\n",
       "      <td>27733652201</td>\n",
       "      <td>BETTER-menu</td>\n",
       "      <td>IN</td>\n",
       "      <td>🌈 FanaFana Fundi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>handled</td>\n",
       "      <td>Moya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>838329 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date                          Contact UUID Contact Name  \\\n",
       "756111 2024-01-17 10:31:51  35ca175a-e695-43ab-b09a-4a7142b9c7e4       friend   \n",
       "349469 2023-12-31 15:07:37  124f3e91-1ec8-4a15-8e31-73a312fa89b8       friend   \n",
       "816143 2024-01-18 20:59:44  de4271c8-91ad-437d-ad5f-71bc6c747815       friend   \n",
       "269993 2023-12-21 20:10:56  5b2d2fcf-5d4e-4fed-9883-9533a54a094f       friend   \n",
       "53587  2023-12-04 05:29:22  71499b9d-e50d-459b-afbe-5066424b0284       friend   \n",
       "...                    ...                                   ...          ...   \n",
       "259444 2023-12-20 17:32:16  61452fd2-ada6-4e24-9dcb-540c4218cb8f       friend   \n",
       "366139 2024-01-02 18:11:30  6ea2207e-ca08-4833-9442-f91d3509ab24       friend   \n",
       "132179 2023-12-09 18:50:33  074b79ef-fc5c-40eb-87f8-3180f00770f0       friend   \n",
       "671513 2024-01-15 11:24:32  48bde2c6-d9a0-43ae-883c-ad1b0c592644       friend   \n",
       "122202 2023-12-08 20:31:25  e836a6f2-7b12-4eb9-bf81-c7372a517c3f       friend   \n",
       "\n",
       "       URN Scheme    URN Value                              Flow Direction  \\\n",
       "756111        tel  27632508008                    AFFIRM-2-part1        IN   \n",
       "349469        tel  27810071650                   SCREENER-normal        IN   \n",
       "816143        tel  27612420701                  Stories-surprise        IN   \n",
       "269993        tel  27640365420                       BETTER-menu        IN   \n",
       "53587         tel  27604023555             BETTER-breathing-menu        IN   \n",
       "...           ...          ...                               ...       ...   \n",
       "259444        tel  27641791761                       BETTER-menu        IN   \n",
       "366139        tel  27648192647                            FORGET        IN   \n",
       "132179        tel  27656610248  Main-Menu-04-ReducedMenu4Options        IN   \n",
       "671513        tel  27676679745  Main-Menu-04-ReducedMenu4Options        IN   \n",
       "122202        tel  27733652201                       BETTER-menu        IN   \n",
       "\n",
       "                    Text Attachments   Status Channel Labels  \n",
       "756111             👍🏽Yes         NaN  handled    Moya    NaN  \n",
       "349469           Tell me         NaN  handled    Moya    NaN  \n",
       "816143           Next 👉🏽         NaN  handled    Moya    NaN  \n",
       "269993             👍🏽Yes         NaN  handled    Moya    NaN  \n",
       "53587              🧠 Box         NaN  handled    Moya    NaN  \n",
       "...                  ...         ...      ...     ...    ...  \n",
       "259444  🌈 FanaFana Fundi         NaN  handled    Moya    NaN  \n",
       "366139           Forget          NaN  handled    Moya    NaN  \n",
       "132179    Read stories 📘         NaN  handled    Moya    NaN  \n",
       "671513  Health quizzes 🧠         NaN  handled    Moya    NaN  \n",
       "122202  🌈 FanaFana Fundi         NaN  handled    Moya    NaN  \n",
       "\n",
       "[838329 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2560)\n",
       "    (wpe): Embedding(2048, 2560)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the GPT-Neo model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering\n",
    "def create_prompt(text):\n",
    "    return f\"Classify the following message for mental health concerns: {text}\"\n",
    "\n",
    "def sentiment_prompt(text):\n",
    "    return f\"Analyze the sentiment of the following message: {text}\"\n",
    "\n",
    "# Real-time Classification and Automated Response\n",
    "def classify_message(message):\n",
    "    prompt = create_prompt(message)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    classification = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return classification\n",
    "\n",
    "def analyze_sentiment(message):\n",
    "    prompt = sentiment_prompt(message)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    sentiment = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return sentiment\n",
    "\n",
    "def automated_response(classification, sentiment):\n",
    "    if 'acute' in classification:\n",
    "        return \"Alerting moderator: Immediate attention needed!\"\n",
    "    elif 'positive' in sentiment:\n",
    "        return \"Thank you for your positive message!\"\n",
    "    else:\n",
    "        return \"We are here to support you. Please reach out if you need help.\"\n",
    "\n",
    "def process_message(message):\n",
    "    classification = classify_message(message)\n",
    "    sentiment = analyze_sentiment(message)\n",
    "    response = automated_response(classification, sentiment)\n",
    "    return response\n",
    "\n",
    "# Alerting Moderators and Personalization\n",
    "def alert_moderator(message, classification):\n",
    "    if 'acute' in classification:\n",
    "        print(f\"Moderator Alert: {message}\")\n",
    "\n",
    "def personalized_response(message, sentiment):\n",
    "    if 'negative' in sentiment:\n",
    "        return f\"We noticed you're feeling down. We're here for you. {message}\"\n",
    "    else:\n",
    "        return f\"Thank you for your message. {message}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for your message. We are here to support you. Please reach out if you need help.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "message = \"I feel really depressed and don't know what to do.\"\n",
    "classification = classify_message(message)\n",
    "sentiment = analyze_sentiment(message)\n",
    "response = automated_response(classification, sentiment)\n",
    "alert_moderator(message, classification)\n",
    "print(personalized_response(response, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for your message. We are here to support you. Please reach out if you need help.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 🧠 Tests\n",
      "Classification: Classify the following message for mental health concerns: 🧠 Tests are normal. 🧠 I am fine. 🧠 I am not fine. 🧠 I am not sure. 🧠 I am not sure. 🧠 I am not sure. 🧠 I am not sure.\n",
      "Sentiment: Analyze the sentiment of the following message: 🧠 Tests are running 🧠\n",
      "\n",
      "The sentiment of the message is positive.\n",
      "\n",
      "The sentiment of the message is negative.\n",
      "\n",
      "The sentiment of the message is neutral.\n",
      "\n",
      "The sentiment of the message is positive.\n",
      "\n",
      "The sentiment of\n",
      "Response: Thank you for your positive message!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: More\n",
      "Classification: Classify the following message for mental health concerns: More than half of the people who died in the U.S. in 2011 were not reported to the National Vital Statistics System, according to a new study.\n",
      "\n",
      "The study, published in the journal Health Affairs, found that more than half of the\n",
      "Sentiment: Analyze the sentiment of the following message: More than a year after the U.S. government shut down, the country is still in a state of uncertainty.\n",
      "\n",
      "The U.S. government shutdown is the longest in U.S. history.\n",
      "\n",
      "The government shutdown is the longest\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: RELAX\n",
      "Classification: Classify the following message for mental health concerns: RELAX.\n",
      "\n",
      "The following message is for mental health concerns: RELAX.\n",
      "\n",
      "The following message is for mental health concerns: RELAX.\n",
      "\n",
      "The following message is for mental health concerns: RELAX.\n",
      "\n",
      "The following message is for\n",
      "Sentiment: Analyze the sentiment of the following message: RELAXED\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relaxed\n",
      "\n",
      "Relax\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: started \n",
      "Classification: Classify the following message for mental health concerns: started __________, stopped __________, and changed __________.\n",
      "\n",
      "**A.**\n",
      "\n",
      "**B.**\n",
      "\n",
      "**C.**\n",
      "\n",
      "**D.**\n",
      "\n",
      "**.** Which of the following is the best\n",
      "Sentiment: Analyze the sentiment of the following message: started \n",
      "\n",
      "Analyze the sentiment of the following message: started\n",
      "\n",
      "Analyze the sentiment of the following message: started\n",
      "\n",
      "Analyze the sentiment of the following message: started\n",
      "\n",
      "Analyze the sentiment of the following message: started\n",
      "\n",
      "\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Hey FanaFana\n",
      "Classification: Classify the following message for mental health concerns: Hey FanaFana, I'm worried about you. I'm worried about you because you're not taking your medication. I'm worried about you because you're not taking your medication. I'm worried about you because you're not taking your medication. I'm worried about\n",
      "Sentiment: Analyze the sentiment of the following message: Hey FanaFana, I'm a new user of your site and I'm really enjoying it. I'm looking forward to reading more of your stories. I hope you don't mind if I send you a message. I'm a new user of your site and I\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: More\n",
      "Classification: Classify the following message for mental health concerns: More than half of the people who died in the U.S. in 2011 were not reported to the National Vital Statistics System, according to a new study.\n",
      "\n",
      "The study, published in the journal Health Affairs, found that more than half of the\n",
      "Sentiment: Analyze the sentiment of the following message: More than a year after the U.S. government shut down, the country is still in a state of uncertainty.\n",
      "\n",
      "The U.S. government shutdown is the longest in U.S. history.\n",
      "\n",
      "The government shutdown is the longest\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: fsg I don't understand what I'm saying \n",
      "Classification: Classify the following message for mental health concerns: fsg I don't understand what I'm saying                                                   \n",
      "Sentiment: Analyze the sentiment of the following message: fsg I don't understand what I'm saying \n",
      "\n",
      "The sentiment of the message is: negative\n",
      "\n",
      "The sentiment of the message is: negative\n",
      "\n",
      "The sentiment of the message is: negative\n",
      "\n",
      "The sentiment of the message is: negative\n",
      "\n",
      "The sentiment of the message is: negative\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 👍🏽Yes\n",
      "Classification: Classify the following message for mental health concerns: 👍🏽Yes, I am worried about my mental health. 👍🏽No, I am not worried about my mental health. 👍🏽I am worried about my mental health. 👍🏽I am not worried about my mental\n",
      "Sentiment: Analyze the sentiment of the following message: 👍🏽Yes, I agree. 👍🏽No, I disagree. 👍🏽I don't know. 👍🏽I don't care. 👍🏽I don't know. 👍🏽I\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 👎🏽No\n",
      "Classification: Classify the following message for mental health concerns: 👎🏽No, I don't have any mental health concerns. 👎🏽No, I don't have any mental health concerns. 👎🏽No, I don't have any mental health concerns. 👎🏽No, I\n",
      "Sentiment: Analyze the sentiment of the following message: 👎🏽No, I don't want to be a part of your team. 👎🏽No, I don't want to be a part of your team. 👎🏽No, I don't want to be a part of your team\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 🔴 Session 1\n",
      "Classification: Classify the following message for mental health concerns: 🔴 Session 1: I am feeling anxious. 🔴 Session 2: I am feeling anxious. 🔴 Session 3: I am feeling anxious. 🔴 Session 4: I am feeling anxious. 🔴 Session 5: I am feeling anxious.\n",
      "Sentiment: Analyze the sentiment of the following message: 🔴 Session 1: 🔴 Session 2: 🔴 Session 3: 🔴 Session 4: 🔴 Session 5: 🔴 Session 6: 🔴 Session 7: 🔴 Session 8: 🔴 Session 9: �\n",
      "Response: We are here to support you. Please reach out if you need help.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt Engineering\n",
    "def create_prompt(text):\n",
    "    return f\"Classify the following message for mental health concerns: {text}\"\n",
    "\n",
    "def sentiment_prompt(text):\n",
    "    return f\"Analyze the sentiment of the following message: {text}\"\n",
    "\n",
    "# Real-time Classification and Automated Response\n",
    "def classify_message(message):\n",
    "    prompt = create_prompt(message)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    classification = tokenizer.decode(outputs[0].to('cpu'), skip_special_tokens=True)  # Move output to CPU\n",
    "    return classification\n",
    "\n",
    "def analyze_sentiment(message):\n",
    "    prompt = sentiment_prompt(message)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    sentiment = tokenizer.decode(outputs[0].to('cpu'), skip_special_tokens=True)  # Move output to CPU\n",
    "    return sentiment\n",
    "\n",
    "def automated_response(classification, sentiment):\n",
    "    if 'acute' in classification:\n",
    "        return \"Alerting moderator: Immediate attention needed!\"\n",
    "    elif 'positive' in sentiment:\n",
    "        return \"Thank you for your positive message!\"\n",
    "    else:\n",
    "        return \"We are here to support you. Please reach out if you need help.\"\n",
    "\n",
    "def process_message(message):\n",
    "    classification = classify_message(message)\n",
    "    sentiment = analyze_sentiment(message)\n",
    "    response = automated_response(classification, sentiment)\n",
    "    return response\n",
    "\n",
    "# Alerting Moderators and Personalization\n",
    "def alert_moderator(message, classification):\n",
    "    if 'acute' in classification:\n",
    "        print(f\"Moderator Alert: {message}\")\n",
    "\n",
    "def personalized_response(message, sentiment):\n",
    "    if 'negative' in sentiment:\n",
    "        return f\"We noticed you're feeling down. We're here for you. {message}\"\n",
    "    else:\n",
    "        return f\"Thank you for your message. {message}\"\n",
    "\n",
    "# Example usage\n",
    "message = \"I feel really depressed and don't know what to do.\"\n",
    "classification = classify_message(message)\n",
    "sentiment = analyze_sentiment(message)\n",
    "response = automated_response(classification, sentiment)\n",
    "alert_moderator(message, classification)\n",
    "print(personalized_response(response, sentiment))\n",
    "\n",
    "# Testing and Optimization\n",
    "def test_chatbot(test_data):\n",
    "    # This function is for demonstration purposes\n",
    "    for _, row in test_data.iterrows():\n",
    "        message = row['Text']\n",
    "        classification = classify_message(message)\n",
    "        sentiment = analyze_sentiment(message)\n",
    "        print(f\"Message: {message}\")\n",
    "        print(f\"Classification: {classification}\")\n",
    "        print(f\"Sentiment: {sentiment}\")\n",
    "        print(f\"Response: {automated_response(classification, sentiment)}\")\n",
    "        print()\n",
    "\n",
    "# Test the chatbot with a subset of test data\n",
    "test_chatbot(test_data.head(10))  # Test with the first 10 rows of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your OpenAI API key\n",
    "openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('/path_to_your_file.xlsx')\n",
    "\n",
    "# Preprocess data\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.dropna(subset=['Text'])  # Drop rows with missing text\n",
    "data['Text'] = data['Text'].astype(str)  # Ensure text column is string type\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define functions to interact with GPT-3.5\n",
    "def create_classification_prompt(text):\n",
    "    return f\"Classify the mental health concern in the following message: {text}\"\n",
    "\n",
    "def create_sentiment_prompt(text):\n",
    "    return f\"Analyze the sentiment of the following message: {text}\"\n",
    "\n",
    "def gpt3_completion(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",  # Use GPT-3.5 model\n",
    "        prompt=prompt,\n",
    "        max_tokens=50,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def classify_message(message):\n",
    "    prompt = create_classification_prompt(message)\n",
    "    classification = gpt3_completion(prompt)\n",
    "    return classification\n",
    "\n",
    "def analyze_sentiment(message):\n",
    "    prompt = create_sentiment_prompt(message)\n",
    "    sentiment = gpt3_completion(prompt)\n",
    "    return sentiment\n",
    "\n",
    "def automated_response(classification, sentiment):\n",
    "    if 'acute' in classification.lower():\n",
    "        return \"Alerting moderator: Immediate attention needed!\"\n",
    "    elif 'positive' in sentiment.lower():\n",
    "        return \"Thank you for your positive message!\"\n",
    "    else:\n",
    "        return \"We are here to support you. Please reach out if you need help.\"\n",
    "\n",
    "def process_message(message):\n",
    "    classification = classify_message(message)\n",
    "    sentiment = analyze_sentiment(message)\n",
    "    response = automated_response(classification, sentiment)\n",
    "    return response\n",
    "\n",
    "# Alerting Moderators and Personalization\n",
    "def alert_moderator(message, classification):\n",
    "    if 'acute' in classification.lower():\n",
    "        print(f\"Moderator Alert: {message}\")\n",
    "\n",
    "def personalized_response(message, sentiment):\n",
    "    if 'negative' in sentiment.lower():\n",
    "        return f\"We noticed you're feeling down. We're here for you. {message}\"\n",
    "    else:\n",
    "        return f\"Thank you for your message. {message}\"\n",
    "\n",
    "# Testing and Optimization\n",
    "def test_chatbot(test_data, batch_size=10):\n",
    "    # Process data in batches\n",
    "    for i in range(0, len(test_data), batch_size):\n",
    "        batch = test_data.iloc[i:i+batch_size]\n",
    "        for _, row in batch.iterrows():\n",
    "            message = row['Text']\n",
    "            classification = classify_message(message)\n",
    "            sentiment = analyze_sentiment(message)\n",
    "            response = automated_response(classification, sentiment)\n",
    "            alert_moderator(message, classification)\n",
    "            print(f\"Message: {message}\")\n",
    "            print(f\"Classification: {classification}\")\n",
    "            print(f\"Sentiment: {sentiment}\")\n",
    "            print(f\"Response: {response}\")\n",
    "            print()\n",
    "\n",
    "# Use a smaller subset for initial testing\n",
    "test_chatbot(test_data.head(100), batch_size=10)  # Test with the first 100 rows of the test set in batches of 10\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
